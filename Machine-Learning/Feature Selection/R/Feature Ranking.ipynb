{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Feature Ranking Approach\n",
    "\n",
    "As we explained, in the ranking approach, features are ranked by some criteria and those which are above a defined threshold are selected. A general algorithm can be considered for such approach where you just need to decide which one if the best ranking criteria to be used. In the F-Selector Package, such criteria is represented by a set of functions that calculate weights to your features according to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(MASS)\n",
    "library(pROC)\n",
    "library(RWeka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             attr_importance\n",
      "Sepal.Length       0.4521286\n",
      "Sepal.Width        0.2672750\n",
      "Petal.Length       0.9402853\n",
      "Petal.Width        0.9554360\n",
      "Species ~ Petal.Width + Petal.Length\n",
      "<environment: 0x5567e80390d0>\n"
     ]
    }
   ],
   "source": [
    "library(FSelector)\n",
    "data(iris)\n",
    "\n",
    "weights <- information.gain(Species~., iris)\n",
    "print(weights)\n",
    "\n",
    "subset <- cutoff.k(weights, 2)\n",
    "\n",
    "f <- as.simple.formula(subset, \"Species\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind FSelector and its functions is to choose the best combination of attributes found in a data set. Maybe, some attributes are unnecesary (maybe), that depends on the dataset you are dealing with.\n",
    "\n",
    "information.gain is a function that select the best combination of attributes according to its \"Information Gain\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        attr_importance\n",
      "STIL         0.22057783\n",
      "MSMB         0.17549995\n",
      "TSPAN13      0.17383600\n",
      "AGR2         0.11872998\n",
      "PECI         0.11438001\n",
      "ABHD12       0.10643349\n",
      "SLC26A2      0.09992717\n",
      "GDF15        0.09812053\n",
      "EIF2B5       0.07533550\n",
      "MAD1L1       0.06429559\n",
      "TM4SF1       0.06213647\n",
      "COL6A32      0.04495090\n",
      "TNFSF10      0.04162775\n"
     ]
    }
   ],
   "source": [
    "#calculate weights for each attribute using some function\n",
    "\n",
    "weights <- information.gain(BCA~., mydata)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             attr_importance\n",
      "Sepal.Length       0.4521286\n",
      "Sepal.Width        0.2672750\n",
      "Petal.Length       0.9402853\n",
      "Petal.Width        0.9554360\n",
      "Species ~ Petal.Width + Petal.Length\n",
      "<environment: 0x5567e8070160>\n",
      "             attr_importance\n",
      "Sepal.Length       0.6522837\n",
      "Sepal.Width        0.3855963\n",
      "Petal.Length       1.3565450\n",
      "Petal.Width        1.3784027\n",
      "             attr_importance\n",
      "Sepal.Length       0.4196464\n",
      "Sepal.Width        0.2472972\n",
      "Petal.Length       0.8584937\n",
      "Petal.Width        0.8713692\n",
      "Species ~ Petal.Width + Petal.Length\n",
      "<environment: 0x5567e65a6488>\n",
      "             attr_importance\n",
      "Sepal.Length       0.4155563\n",
      "Sepal.Width        0.2452743\n",
      "Petal.Length       0.8571872\n",
      "Petal.Width        0.8705214\n",
      "Species ~ Petal.Width + Petal.Length\n",
      "<environment: 0x5567e7c528e8>\n"
     ]
    }
   ],
   "source": [
    "data(iris)\n",
    "\n",
    "  weights <- information.gain(Species~., iris)\n",
    "  print(weights)\n",
    "  subset <- cutoff.k(weights, 2)\n",
    "  f <- as.simple.formula(subset, \"Species\")\n",
    "  print(f)\n",
    "\n",
    "  weights <- information.gain(Species~., iris, unit = \"log2\")\n",
    "  print(weights)\n",
    "\n",
    "  weights <- gain.ratio(Species~., iris)\n",
    "  print(weights)\n",
    "  subset <- cutoff.k(weights, 2)\n",
    "  f <- as.simple.formula(subset, \"Species\")\n",
    "  print(f)\n",
    "\n",
    "  weights <- symmetrical.uncertainty(Species~., iris)\n",
    "  print(weights)\n",
    "  subset <- cutoff.biggest.diff(weights)\n",
    "  f <- as.simple.formula(subset, \"Species\")\n",
    "  print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection using Information Gain in R\n",
    "\n",
    "\n",
    "When considering a predictive model, you might be interested in knowing which features of your data provide the most information about the target variable of interest. For example, suppose we’d like to predict the species of Iris based on sepal length and width as well as petal length and width (using the iris dataset in R).\n",
    "\n",
    "Which of these 4 features provides the “purest” segmentation with respect to the target? Or put differently, if you were to place a bet on the correct species, and could only ask for the value of 1 feature, which feature would give you the greatest likelihood of winning your bet?\n",
    "\n",
    "While there are many R packages out there for attribute selection, I’ve coded a few basic functions for my own usage for selecting attributes based on Information Gain (and hence on Shannon Entropy).\n",
    "\n",
    "For starters, let’s define what we mean by Entropy and Information Gain.\n",
    "\n",
    "$$\n",
    "\\begin{array} { c } { \\text { Shannon Entropy } } \\\\ { H \\left( p _ { 1 } \\ldots p _ { n } \\right) = \\sum _ { i = 1 } ^ { n } p _ { i } \\log _ { 2 } p _ { i } } \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "Where $p_i$ is the probability of value i and n is the number of possible values. For example in the iris dataset, we have 3 possible values for Species (Setosa, Versicolor, Virginica), each representing $\\frac{1}{3}$ of the data. Therefore\n",
    "\n",
    "$$\n",
    "\\sum _ { i = 1 } ^ { 3 } \\frac { 1 } { 3 } i \\log _ { 2 } \\frac { 1 } { 3 } _ { i } = 1.59\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array} { c } { \\text { Information Gain } } \\\\ { I G = H _ { p } - \\sum _ { i = 1 } ^ { n } p _ { c i } H _ { c i } } \\end{array}\n",
    "$$\n",
    "\n",
    "Where $H_p$ is the entropy of the parent (the complete, unsegmented dataset), n  is the number of values of our target variable (and the number of child segments), $p_{ci}$ is the probability that an observation is in child i (the weighting), and $H_{ci}$ is the entropy of child (segment) i.\n",
    "\n",
    "Continuing with our iris example, we could ask the following: “Can we improve (reduce) the entropy of the parent dataset by segmenting on Sepal Length?”\n",
    "\n",
    "In this case, Sepal Length is numeric. You’ll notice the code provides functions for both numeric and categorical variables. For categorical variables, we simply segment on each possible value. However in the numeric case, we will bin the data according to the desired number of breaks (which is set to 4 by default).\n",
    "\n",
    "If we segment using 5 breaks, we get 5 children. Note e is the computed entropy for this subset, p is the proportion of records, N is the number of records, and min and max are… the min and max.\n",
    "\n",
    "We improve on the entropy of the parent in each child. In fact, segment 5 is perfectly pure, though weighted lightly due to the low proportion of records it contains. We can formalize this using the information gain formula noted above. Calling the IG_numeric function, we see the that IG(Sepal.Length) = .64 using 5 breaks.\n",
    "\n",
    "Note that the categorical and numeric functions are called as follows\n",
    "\n",
    "IG_numeric(data, feature, target, bins=4)\n",
    "\n",
    "IG_cat(data,feature,target)\n",
    "\n",
    "Both functions return the IG value, however you can change return(IG) to return(dd_data) to return the summary of the segments as a data.frame for investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_process <-function(classes,splitvar = NULL){\n",
    "  #Assumes Splitvar is a logical vector\n",
    "  if (is.null(splitvar)){\n",
    "    base_prob <-table(classes)/length(classes)\n",
    "    return(1-sum(base_prob**2))\n",
    "  }\n",
    "  base_prob <-table(splitvar)/length(splitvar)\n",
    "  crosstab <- table(classes,splitvar)\n",
    "  crossprob <- prop.table(crosstab,2)\n",
    "  No_Node_Gini <- 1-sum(crossprob[,1]**2)\n",
    "  Yes_Node_Gini <- 1-sum(crossprob[,2]**2)\n",
    "  return(sum(base_prob * c(No_Node_Gini,Yes_Node_Gini)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.666666666666667"
      ],
      "text/latex": [
       "0.666666666666667"
      ],
      "text/markdown": [
       "0.666666666666667"
      ],
      "text/plain": [
       "[1] 0.6666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.333333333333333"
      ],
      "text/latex": [
       "0.333333333333333"
      ],
      "text/markdown": [
       "0.333333333333333"
      ],
      "text/plain": [
       "[1] 0.3333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.408584169453735"
      ],
      "text/latex": [
       "0.408584169453735"
      ],
      "text/markdown": [
       "0.408584169453735"
      ],
      "text/plain": [
       "[1] 0.4085842"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.557760141093474"
      ],
      "text/latex": [
       "0.557760141093474"
      ],
      "text/markdown": [
       "0.557760141093474"
      ],
      "text/plain": [
       "[1] 0.5577601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(iris)\n",
    "gini_process(iris$Species) #0.6667\n",
    "gini_process(iris$Species,iris$Petal.Length<2.45) #0.3333\n",
    "gini_process(iris$Species,iris$Petal.Length<5) #0.4086\n",
    "gini_process(iris$Species,iris$Sepal.Length<6.4) #0.5578"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
